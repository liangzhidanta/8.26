#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯è§†åŒ–SVDå’Œç¥ç»ç½‘ç»œä½ç§©åˆ†è§£çš„æ¯”è¾ƒç»“æœ
å‚è€ƒvisualize_comparison.pyçš„æ–¹å¼ï¼Œä½¿ç”¨ç©ºé—´å¸ƒå±€å±•ç¤ºå‰8ä¸ªé€šé“
"""

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image
import argparse

from qwenvl_dataloader import RealQwenVLDataset, create_data_loaders_real
from model import LowRankProjSymmetric
from svd_baseline import svd_truncated_error, svd_truncated_reconstruction


def reshape_patches_to_spatial(X: torch.Tensor, image_size: int = 448, patch_size: int = 16) -> torch.Tensor:
    """
    å°†patchåºåˆ—é‡æ–°æ’åˆ—ä¸ºç©ºé—´å¸ƒå±€
    X: (T, D) -> (H, W, D) å…¶ä¸­ H*W = T
    å¯¹äº448x448å›¾åƒï¼Œ16x16 patchï¼Œåº”è¯¥æœ‰256ä¸ªtokenï¼Œå¯¹åº”16x16çš„ç©ºé—´å¸ƒå±€
    """
    T, D = X.shape
    
    # è®¡ç®—ç©ºé—´å°ºå¯¸
    if T == 256:
        # 256 = 16*16ï¼Œå¯¹åº”16x16çš„ç©ºé—´å¸ƒå±€
        H, W = 16, 16
    elif T == 784:
        # 784 = 28*28ï¼Œå¯¹åº”28x28çš„ç©ºé—´å¸ƒå±€ï¼ˆ448/16 = 28ï¼‰
        H, W = 28, 28
    elif T == 196:
        # 196 = 14*14
        H, W = 14, 14
    else:
        # å°è¯•æ‰¾åˆ°æœ€æ¥è¿‘çš„å¹³æ–¹æ•°
        side = int(np.sqrt(T))
        H, W = side, side
    
    # å¦‚æœTä¸ç­‰äºH*Wï¼Œè¿›è¡Œpaddingæˆ–truncation
    if T < H * W:
        # éœ€è¦padding
        pad_size = H * W - T
        X_padded = torch.cat([X, torch.zeros(pad_size, D, device=X.device, dtype=X.dtype)], dim=0)
        return X_padded.view(H, W, D)
    elif T > H * W:
        # éœ€è¦truncation
        return X[:H*W].view(H, W, D)
    else:
        return X.view(H, W, D)


def visualize_channels_spatial(X: torch.Tensor, X_hat: torch.Tensor, X_svd: torch.Tensor, 
                              out_dir: str, image_idx: int = 0, image_size: int = 448, patch_size: int = 16) -> None:
    """å¯è§†åŒ–å‰8ä¸ªchannelçš„ç‰¹å¾çŸ©é˜µï¼Œæ¯ä¸ªchannelæ˜¾ç¤ºä¸º16x16çš„ç©ºé—´å¸ƒå±€"""
    os.makedirs(out_dir, exist_ok=True)
    
    # å–å‰8ä¸ªchannel
    num_channels = min(8, X.shape[-1])
    
    # é‡å¡‘ä¸ºç©ºé—´å¸ƒå±€
    X_spatial = reshape_patches_to_spatial(X, image_size, patch_size)  # (H, W, D)
    X_hat_spatial = reshape_patches_to_spatial(X_hat, image_size, patch_size)
    X_svd_spatial = reshape_patches_to_spatial(X_svd, image_size, patch_size)
    
    # åˆ›å»º3è¡Œ8åˆ—çš„å­å›¾å¸ƒå±€
    fig, axes = plt.subplots(3, num_channels, figsize=(20, 12))
    
    # ä¸ºæ¯ä¸ªchannelåˆ›å»ºå¯è§†åŒ–
    for ch in range(num_channels):
        # åŸç‰¹å¾çŸ©é˜µ
        im1 = axes[0, ch].imshow(X_spatial[:, :, ch].detach().cpu().numpy(), 
                                 aspect='equal', cmap='viridis')  # æ”¹ä¸ºæ­£æ–¹å½¢å±•ç¤º
        axes[0, ch].set_title(f'Ch{ch+1}', fontsize=12)
        axes[0, ch].set_ylabel('Original', fontsize=12, fontweight='bold')
        
        # ç¥ç»ç½‘ç»œé‡æ„
        im2 = axes[1, ch].imshow(X_hat_spatial[:, :, ch].detach().cpu().numpy(), 
                                 aspect='equal', cmap='viridis')  # æ”¹ä¸ºæ­£æ–¹å½¢å±•ç¤º
        axes[1, ch].set_ylabel('Neural Net', fontsize=12, fontweight='bold')
        
        # SVDé‡æ„
        im3 = axes[2, ch].imshow(X_svd_spatial[:, :, ch].detach().cpu().numpy(), 
                                 aspect='equal', cmap='viridis')  # æ”¹ä¸ºæ­£æ–¹å½¢å±•ç¤º
        axes[2, ch].set_xlabel('Width', fontsize=10)
        axes[2, ch].set_ylabel('SVD', fontsize=12, fontweight='bold')
        
        # è®¾ç½®åˆ»åº¦
        axes[0, ch].set_xticks([])
        axes[1, ch].set_xticks([])
        axes[2, ch].set_xticks([])
        
        # åªåœ¨æœ€å·¦è¾¹æ˜¾ç¤ºyè½´æ ‡ç­¾
        if ch > 0:
            axes[0, ch].set_yticks([])
            axes[1, ch].set_yticks([])
            axes[2, ch].set_yticks([])
    
    # æ·»åŠ æ€»æ ‡é¢˜
    fig.suptitle(f'Spatial Feature Comparison (Image {image_idx}, Image: {image_size}x{image_size}, Patch: {patch_size}x{patch_size})', 
                 fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, f'image_{image_idx}_spatial_channel_comparison.png'), 
                dpi=150, bbox_inches='tight')
    plt.close()


def visualize_differences_spatial(X: torch.Tensor, X_hat: torch.Tensor, X_svd: torch.Tensor, 
                                 out_dir: str, image_idx: int = 0, image_size: int = 448, patch_size: int = 16) -> None:
    """å¯è§†åŒ–é‡æ„è¯¯å·®ï¼Œæ¯ä¸ªchannelæ˜¾ç¤ºä¸º16x16çš„ç©ºé—´å¸ƒå±€"""
    os.makedirs(out_dir, exist_ok=True)
    
    # è®¡ç®—è¯¯å·®
    err_nn = (X - X_hat).detach()
    err_svd = (X - X_svd).detach()
    
    # å–å‰8ä¸ªchannel
    num_channels = min(8, X.shape[-1])
    
    # é‡å¡‘ä¸ºç©ºé—´å¸ƒå±€
    err_nn_spatial = reshape_patches_to_spatial(err_nn, image_size, patch_size)
    err_svd_spatial = reshape_patches_to_spatial(err_svd, image_size, patch_size)
    
    # åˆ›å»º2è¡Œ8åˆ—çš„å­å›¾å¸ƒå±€
    fig, axes = plt.subplots(2, num_channels, figsize=(20, 8))
    
    # ä¸ºæ¯ä¸ªchannelåˆ›å»ºè¯¯å·®å¯è§†åŒ–
    for ch in range(num_channels):
        # ç¥ç»ç½‘ç»œé‡æ„è¯¯å·®
        im1 = axes[0, ch].imshow(np.abs(err_nn_spatial[:, :, ch].cpu().numpy()), 
                                 aspect='equal', cmap='magma')  # æ”¹ä¸ºæ­£æ–¹å½¢å±•ç¤º
        axes[0, ch].set_title(f'Ch{ch+1}', fontsize=12)
        if ch == 0:
            axes[0, ch].set_ylabel('Neural Net Error', fontsize=12, fontweight='bold')
        
        # SVDé‡æ„è¯¯å·®
        im2 = axes[1, ch].imshow(np.abs(err_svd_spatial[:, :, ch].cpu().numpy()), 
                                 aspect='equal', cmap='magma')  # æ”¹ä¸ºæ­£æ–¹å½¢å±•ç¤º
        if ch == 0:
            axes[1, ch].set_ylabel('SVD Error', fontsize=12, fontweight='bold')
        
        # è®¾ç½®åˆ»åº¦
        axes[0, ch].set_xticks([])
        axes[1, ch].set_xticks([])
        
        # åªåœ¨æœ€å·¦è¾¹æ˜¾ç¤ºyè½´æ ‡ç­¾
        if ch > 0:
            axes[0, ch].set_yticks([])
            axes[1, ch].set_yticks([])
    
    # æ·»åŠ æ€»æ ‡é¢˜
    fig.suptitle(f'Spatial Reconstruction Error (Image {image_idx}, Image: {image_size}x{image_size}, Patch: {patch_size}x{patch_size})', 
                 fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, f'image_{image_idx}_spatial_error_comparison.png'), 
                dpi=150, bbox_inches='tight')
    plt.close()


def compute_cosine_similarity(X_original, X_reconstructed):
    """è®¡ç®—é‡æ„çŸ©é˜µä¸åŸå§‹çŸ©é˜µçš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    # å°†å¼ é‡å±•å¹³ä¸º2DçŸ©é˜µ [batch_size, tokens*dim]
    X_flat = X_original.view(X_original.size(0), -1)
    X_recon_flat = X_reconstructed.view(X_reconstructed.size(0), -1)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = []
    for i in range(X_flat.size(0)):
        sim = np.dot(X_flat[i].cpu().numpy(), X_recon_flat[i].cpu().numpy()) / (
            np.linalg.norm(X_flat[i].cpu().numpy()) * np.linalg.norm(X_recon_flat[i].cpu().numpy()) + 1e-8
        )
        similarities.append(sim)
    
    return np.mean(similarities)


def visualize_single_image_comparison(dataset, model, rank, device, results_dir, image_idx=0):
    """å¯è§†åŒ–å•å¼ å›¾åƒçš„é‡æ„æ¯”è¾ƒ - ä½¿ç”¨ç©ºé—´å¸ƒå±€å±•ç¤ºå‰8ä¸ªé€šé“"""
    print(f"ğŸ¨ å¯è§†åŒ–å›¾åƒ {image_idx} çš„é‡æ„æ¯”è¾ƒ...")
    
    # è·å–å›¾åƒæ•°æ®
    X = dataset[image_idx][0].unsqueeze(0)  # [1, 256, 2048]
    print(f"ğŸ“Š åŸå§‹ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    
    # 1. SVDé‡æ„
    print("ğŸ“ˆ è®¡ç®—SVDé‡æ„...")
    X_svd_recon = svd_truncated_reconstruction(X.squeeze(0), rank).unsqueeze(0)
    
    # 2. ç¥ç»ç½‘ç»œé‡æ„
    print("ğŸ§  è®¡ç®—ç¥ç»ç½‘ç»œé‡æ„...")
    model.eval()
    with torch.no_grad():
        X_norm = layernorm_along_feature(X.to(device))
        X_hat, A, B = model(X_norm)
        X_nn_recon = X_hat
    
    print(f"âœ… é‡æ„å®Œæˆ:")
    print(f"  SVDé‡æ„å½¢çŠ¶: {X_svd_recon.shape}")
    print(f"  NNé‡æ„å½¢çŠ¶: {X_nn_recon.shape}")
    
    # 3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    svd_sim = compute_cosine_similarity(X, X_svd_recon)
    nn_sim = compute_cosine_similarity(X, X_nn_recon)
    
    print(f"ğŸ“Š ä½™å¼¦ç›¸ä¼¼åº¦:")
    print(f"  SVD: {svd_sim:.4f}")
    print(f"  NN:  {nn_sim:.4f}")
    
    # 4. ç”Ÿæˆç©ºé—´å¸ƒå±€çš„å¯è§†åŒ–
    print("ğŸ¨ ç”Ÿæˆç©ºé—´å¸ƒå±€å¯è§†åŒ–...")
    try:
        # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨CPUä¸Š
        X_cpu = X.squeeze(0).cpu()
        X_nn_recon_cpu = X_nn_recon.squeeze(0).cpu()
        X_svd_recon_cpu = X_svd_recon.squeeze(0).cpu()
        
        # ä½¿ç”¨ç©ºé—´å¸ƒå±€å±•ç¤ºå‰8ä¸ªé€šé“
        visualize_channels_spatial(X_cpu, X_nn_recon_cpu, X_svd_recon_cpu, 
                                 results_dir, image_idx, image_size=448, patch_size=16)
        visualize_differences_spatial(X_cpu, X_nn_recon_cpu, X_svd_recon_cpu, 
                                    results_dir, image_idx, image_size=448, patch_size=16)
        print(f"âœ… ç©ºé—´å¸ƒå±€å¯è§†åŒ–ç”Ÿæˆå®Œæˆ")
    except Exception as e:
        print(f"âš ï¸  ç©ºé—´å¸ƒå±€å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        print("å›é€€åˆ°åŸå§‹çƒ­åŠ›å›¾å¯è§†åŒ–...")
        
        # å›é€€åˆ°åŸå§‹çš„çƒ­åŠ›å›¾å¯è§†åŒ–
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'Image {image_idx} Reconstruction Comparison (Rank={rank})', fontsize=16)
        
        # åŸå§‹ç‰¹å¾çŸ©é˜µçƒ­åŠ›å›¾
        im1 = axes[0, 0].imshow(X[0].cpu().numpy(), cmap='viridis', aspect='auto')
        axes[0, 0].set_title('Original Features')
        axes[0, 0].set_xlabel('Feature Dimension')
        axes[0, 0].set_ylabel('Token Position')
        plt.colorbar(im1, ax=axes[0, 0])
        
        # SVDé‡æ„çƒ­åŠ›å›¾
        im2 = axes[0, 1].imshow(X_svd_recon[0].cpu().numpy(), cmap='viridis', aspect='auto')
        axes[0, 1].set_title(f'SVD Reconstruction\n(Similarity: {svd_sim:.4f})')
        axes[0, 1].set_xlabel('Feature Dimension')
        axes[0, 1].set_ylabel('Token Position')
        plt.colorbar(im2, ax=axes[0, 1])
        
        # ç¥ç»ç½‘ç»œé‡æ„çƒ­åŠ›å›¾
        im3 = axes[0, 2].imshow(X_nn_recon[0].cpu().numpy(), cmap='viridis', aspect='auto')
        axes[0, 2].set_title(f'Neural Network Reconstruction\n(Similarity: {nn_sim:.4f})')
        axes[0, 2].set_xlabel('Feature Dimension')
        axes[0, 2].set_ylabel('Token Position')
        plt.colorbar(im3, ax=axes[0, 2])
        
        # é‡æ„è¯¯å·®çƒ­åŠ›å›¾
        svd_error = torch.abs(X - X_svd_recon)[0].cpu().numpy()
        nn_error = torch.abs(X - X_nn_recon)[0].cpu().numpy()
        
        # è®¡ç®—è¯¯å·®çš„ç»Ÿè®¡ä¿¡æ¯ç”¨äºæ˜¾ç¤º
        svd_error_min, svd_error_max = svd_error.min(), svd_error.max()
        nn_error_min, nn_error_max = nn_error.min(), nn_error.max()
        
        im4 = axes[1, 0].imshow(svd_error, cmap='Reds', aspect='auto', vmin=0, vmax=svd_error_max)
        axes[1, 0].set_title(f'SVD Reconstruction Error\nMin: {svd_error_min:.4f}, Max: {svd_error_max:.4f}')
        axes[1, 0].set_xlabel('Feature Dimension')
        axes[1, 0].set_ylabel('Token Position')
        plt.colorbar(im4, ax=axes[1, 0])
        
        im5 = axes[1, 1].imshow(nn_error, cmap='Reds', aspect='auto', vmin=0, vmax=nn_error_max)
        axes[1, 1].set_title(f'NN Reconstruction Error\nMin: {nn_error_min:.4f}, Max: {nn_error_max:.4f}')
        axes[1, 1].set_xlabel('Feature Dimension')
        axes[1, 1].set_ylabel('Token Position')
        plt.colorbar(im5, ax=axes[1, 1])
        
        # ç›¸ä¼¼åº¦å¯¹æ¯”æ¡å½¢å›¾
        methods = ['SVD', 'Neural Network']
        similarities = [svd_sim, nn_sim]
        colors = ['blue', 'red']
        
        bars = axes[1, 2].bar(methods, similarities, color=colors, alpha=0.7)
        axes[1, 2].set_title('Cosine Similarity Comparison')
        axes[1, 2].set_ylabel('Cosine Similarity')
        axes[1, 2].set_ylim(0, 1)
        axes[1, 2].grid(True, alpha=0.3)
        
        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, sim in zip(bars, similarities):
            height = bar.get_height()
            axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                            f'{sim:.4f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        vis_path = os.path.join(results_dir, f"image_{image_idx}_reconstruction_comparison.png")
        plt.savefig(vis_path, dpi=300, bbox_inches='tight')
        print(f"âœ… çƒ­åŠ›å›¾å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {vis_path}")
    
    # ä¿å­˜æ•°å€¼ç»“æœ
    results = {
        'image_idx': image_idx,
        'rank': rank,
        'similarities': {
            'svd': float(svd_sim),
            'neural_network': float(nn_sim)
        },
        'shapes': {
            'original': X.shape,
            'svd_reconstruction': X_svd_recon.shape,
            'nn_reconstruction': X_nn_recon.shape
        }
    }
    
    import json
    results_path = os.path.join(results_dir, f"image_{image_idx}_comparison_results.json")
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"âœ… æ•°å€¼ç»“æœä¿å­˜åˆ°: {results_path}")
    
    return results


def layernorm_along_feature(X: torch.Tensor) -> torch.Tensor:
    """æ²¿ç€ç‰¹å¾ç»´åº¦è¿›è¡Œå±‚å½’ä¸€åŒ–"""
    return torch.nn.functional.layer_norm(X, normalized_shape=(X.size(-1),))


def main():
    parser = argparse.ArgumentParser(description='Visualize SVD vs Neural Network reconstruction')
    parser.add_argument('--rank', type=int, default=128, help='Rank for decomposition')
    parser.add_argument('--results_dir', type=str, default='results', help='Results directory')
    parser.add_argument('--image_idx', type=int, default=0, help='Image index to visualize')
    
    args = parser.parse_args()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = os.path.join(args.results_dir, "best_model.pt")
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹æ–‡ä»¶")
        return
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    dataset = RealQwenVLDataset(use_cache=True, cache_dir="./feature_cache_real")
    
    if args.image_idx >= len(dataset):
        print(f"âŒ å›¾åƒç´¢å¼• {args.image_idx} è¶…å‡ºèŒƒå›´ (0-{len(dataset)-1})")
        return
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    print("ğŸ§  åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    model = LowRankProjSymmetric(input_dim=2048, num_tokens=256, rank=args.rank, normalize=False)
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state'])
    model.to(device)
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    results = visualize_single_image_comparison(dataset, model, args.rank, device, args.results_dir, args.image_idx)
    
    print(f"\nğŸ‰ å¯è§†åŒ–å®Œæˆï¼")
    print(f"ğŸ“Š ç»“æœ:")
    print(f"  SVDç›¸ä¼¼åº¦: {results['similarities']['svd']:.4f}")
    print(f"  NNç›¸ä¼¼åº¦:  {results['similarities']['neural_network']:.4f}")


if __name__ == "__main__":
    main()
