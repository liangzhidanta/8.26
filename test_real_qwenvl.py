#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•çœŸå®QwenVLæ•°æ®é›†åŠ è½½å™¨ï¼ˆå°æ ·æœ¬ç‰ˆæœ¬ï¼‰
"""

import os
import torch
import time

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["MODELSCOPE_CACHE"] = "/data2/dzr/.cache" 
os.environ["TOKENS_PARALLELISM"] = "false"

def test_real_qwenvl_small():
    """æµ‹è¯•çœŸå®QwenVLæ•°æ®é›†åŠ è½½å™¨ï¼ˆå°æ ·æœ¬ï¼‰"""
    print("=" * 60)
    print("Testing Real QwenVL Dataset Loader (Small Sample)")
    print("=" * 60)
    
    try:
        # å¯¼å…¥æ•°æ®é›†åŠ è½½å™¨
        from qwenvl_dataloader import RealQwenVLDataset
        
        print("âœ… Real QwenVL dataset loader imported successfully")
        
        # åˆ›å»ºä¸€ä¸ªé™åˆ¶æ ·æœ¬æ•°çš„æ•°æ®é›†ç±»
        class LimitedRealQwenVLDataset(RealQwenVLDataset):
            def _preprocess_data(self):
                """åªå¤„ç†å‰10ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•"""
                print("Preprocessing limited data (first 10 samples)...")
                
                self.features = []
                self.valid_indices = []
                
                max_samples = 10  # åªå¤„ç†å‰10ä¸ªæ ·æœ¬
                
                for idx, row in self.df.iterrows():
                    if idx >= max_samples:
                        break
                        
                    print(f"Processing sample {idx+1}/{max_samples}")
                    
                    # è·å–å›¾åƒ
                    image = self._bytes_to_image(row['image'])
                    if image is None:
                        continue
                    
                    # Resizeå›¾åƒ
                    image = self._resize_image(image)
                    
                    # æå–ç‰¹å¾
                    try:
                        features = self._get_image_features(image)
                        self.features.append(features)
                        self.valid_indices.append(idx)
                        print(f"  âœ… Sample {idx+1} processed successfully")
                    except Exception as e:
                        print(f"  âŒ Sample {idx+1} failed: {e}")
                        continue
                
                print(f"âœ… Limited preprocessing completed! Valid samples: {len(self.features)}")
                
                # è½¬æ¢ä¸ºtensor
                if self.features:
                    self.features = torch.cat(self.features, dim=0)  # [N, 256, 2048]
                    print(f"Features tensor shape: {self.features.shape}")
                else:
                    raise RuntimeError("No valid features extracted")
        
        # æµ‹è¯•æ•°æ®é›†åŠ è½½ï¼ˆåªåŠ è½½å‰10ä¸ªæ ·æœ¬ï¼‰
        print("\n1. Testing dataset loading...")
        start_time = time.time()
        
        dataset = LimitedRealQwenVLDataset(
            dataset_dir="/data2/dzr/textVQA_groundingtask_bbox",
            image_size=448,
            use_cache=False,  # ç¦ç”¨ç¼“å­˜è¿›è¡Œæµ‹è¯•
            cache_dir="./feature_cache_test"
        )
        
        load_time = time.time() - start_time
        print(f"âœ… Dataset loaded in {load_time:.2f}s")
        print(f"   Total samples: {len(dataset)}")
        
        # æµ‹è¯•ç‰¹å¾å½¢çŠ¶
        print("\n2. Testing feature extraction...")
        sample_features = dataset[0]
        print(f"âœ… Sample features shape: {sample_features[0].shape}")
        print(f"   Expected: [256, 2048]")
        print(f"   Actual: {list(sample_features[0].shape)}")
        
        # æµ‹è¯•æ•°æ®åˆ’åˆ†
        print("\n3. Testing dataset splitting...")
        train_set, val_set, test_set = dataset.get_split_datasets(
            train_ratio=0.8, 
            val_ratio=0.1, 
            test_ratio=0.1
        )
        print(f"âœ… Dataset split successful:")
        print(f"   Train: {len(train_set)} samples")
        print(f"   Val: {len(val_set)} samples") 
        print(f"   Test: {len(test_set)} samples")
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨åˆ›å»º
        print("\n4. Testing data loader creation...")
        from qwenvl_dataloader import create_data_loaders_real
        train_loader, val_loader, test_loader = create_data_loaders_real(
            dataset, 
            batch_size=2,
            train_ratio=0.8,
            val_ratio=0.1,
            test_ratio=0.1,
            num_workers=0  # ä½¿ç”¨å•è¿›ç¨‹é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        print(f"âœ… Data loaders created successfully")
        
        # æµ‹è¯•ä¸€ä¸ªbatch
        print("\n5. Testing batch loading...")
        for batch in train_loader:
            X = batch[0]
            print(f"âœ… Batch loaded successfully:")
            print(f"   Batch shape: {X.shape}")
            print(f"   Expected: [2, 256, 2048]")
            print(f"   Actual: {list(X.shape)}")
            break
        
        # æµ‹è¯•è®¾å¤‡å…¼å®¹æ€§
        print("\n6. Testing device compatibility...")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"   Using device: {device}")
        
        if torch.cuda.is_available():
            print(f"   GPU: {torch.cuda.get_device_name()}")
            print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        # æµ‹è¯•ç‰¹å¾ç§»åŠ¨åˆ°è®¾å¤‡
        X_device = X.to(device)
        print(f"âœ… Features moved to {device} successfully")
        print(f"   Device tensor shape: {X_device.shape}")
        
        print("\n" + "="*60)
        print("âœ… All tests passed! Real QwenVL dataset loader is working correctly.")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("Starting real QwenVL dataset test (small sample)...")
    
    # è¿è¡Œæµ‹è¯•
    success = test_real_qwenvl_small()
    
    if success:
        print("\nğŸ‰ Test completed successfully!")
        print("You can now proceed with full training using:")
        print("python train_real_data.py")
    else:
        print("\nâŒ Test failed. Please check the error messages above.")
    
    print("\nTest completed!")
