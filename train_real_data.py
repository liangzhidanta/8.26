#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½ç§©åˆ†è§£è®­ç»ƒè„šæœ¬ - ä½¿ç”¨çœŸå®QwenVLæ•°æ®
æ”¯æŒæ—©åœæœºåˆ¶å’Œè¶…å‚æ•°é…ç½®
"""

import os
import argparse
os.environ["MODELSCOPE_CACHE"] = "/data2/dzr/.cache" 
os.environ["TOKENS_PARALLELISM"] = "false"
import math
import time
import random
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from typing import Dict, Any
import shutil

from qwenvl_dataloader import RealQwenVLDataset, create_data_loaders_real
from model import LowRankProjSymmetric, ResidualMLP
from svd_baseline import svd_truncated_error


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True


def layernorm_along_feature(X: torch.Tensor) -> torch.Tensor:
    """æ²¿ç€ç‰¹å¾ç»´åº¦è¿›è¡Œå±‚å½’ä¸€åŒ–"""
    return torch.nn.functional.layer_norm(X, normalized_shape=(X.size(-1),))


def train_one_epoch(model, dataloader, optimizer, device, lambda_B, lambda_ortho):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    total_mse = 0.0
    n_batches = 0
    first_batch = True
    
    # æ·»åŠ è¿›åº¦æ¡
    from tqdm import tqdm
    pbar = tqdm(dataloader, desc="Training", leave=False)
    
    for (X,) in pbar:
        if first_batch:
            print(f"ğŸ” DEBUG: Input tensor X shape (before transfer): {X.shape}")
            print(f"ğŸ” DEBUG: Input tensor X dtype (before transfer): {X.dtype}")
            print(f"ğŸ” DEBUG: Input tensor X device (before transfer): {X.device}")
            print(f"ğŸ” DEBUG: Input tensor X min/max (before transfer): {X.min().item():.4f}/{X.max().item():.4f}")
            print(f"ğŸ” DEBUG: Input tensor X mean/std (before transfer): {X.mean().item():.4f}/{X.std().item():.4f}")
            first_batch = False
            
        # æ˜ç¡®åœ°å°†æ•°æ®ç§»åŠ¨åˆ°GPU
        X = X.to(device, non_blocking=True)
        if n_batches == 0:
            print(f"ğŸ” DEBUG: Input tensor X device (after transfer): {X.device}")
        
        X = layernorm_along_feature(X)
        
        if n_batches == 0:
            print(f"ğŸ” DEBUG: After layernorm X shape: {X.shape}")
            print(f"ğŸ” DEBUG: After layernorm X device: {X.device}")
            print(f"ğŸ” DEBUG: After layernorm X min/max: {X.min().item():.4f}/{X.max().item():.4f}")
            print(f"ğŸ” DEBUG: After layernorm X mean/std: {X.mean().item():.4f}/{X.std().item():.4f}")
        
        optimizer.zero_grad(set_to_none=True)
        X_hat, A, B = model(X)
        
        if n_batches == 0:
            print(f"ğŸ” DEBUG: Model output X_hat shape: {X_hat.shape}")
            print(f"ğŸ” DEBUG: Model output A shape: {A.shape}")
            print(f"ğŸ” DEBUG: Model output B shape: {B.shape}")
        
        # ä»…ä½¿ç”¨MSEæŸå¤±è¿›è¡Œä¼˜åŒ–
        mse = torch.mean((X - X_hat) ** 2)
        loss = mse
        
        if n_batches == 0:
            print(f"ğŸ” DEBUG: MSE (reconstruction error): {mse.item():.6f}")
            print(f"ğŸ” DEBUG: Total loss (MSE only): {loss.item():.6f}")
        
        loss.backward()
        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        optimizer.step()
        
        total_loss += loss.item()
        total_mse += mse.item()
        n_batches += 1
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰loss
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'mse': f'{mse.item():.4f}'
        })
    
    pbar.close()
    return {
        "loss": total_loss / max(n_batches, 1),
        "mse": total_mse / max(n_batches, 1),
    }


def evaluate(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_mse = 0.0
    total_rel_f = 0.0
    total_evr = 0.0
    n_batches = 0
    
    # æ·»åŠ è¿›åº¦æ¡
    from tqdm import tqdm
    pbar = tqdm(dataloader, desc="Evaluating", leave=False)
    
    with torch.no_grad():
        for (X,) in pbar:
            # æ˜ç¡®åœ°å°†æ•°æ®ç§»åŠ¨åˆ°GPU
            X = X.to(device, non_blocking=True)
            X = layernorm_along_feature(X)
            X_hat, A, B = model(X)
            
            # MSE
            mse = torch.mean((X - X_hat) ** 2)
            total_mse += mse.item()
            
            # Rel_F
            rel_f = torch.mean(torch.norm(X - X_hat, dim=-1) / (torch.norm(X, dim=-1) + 1e-8))
            total_rel_f += rel_f.item()
            
            # EVR
            evr = torch.mean(torch.norm(X_hat, dim=-1) / (torch.norm(X, dim=-1) + 1e-8))
            total_evr += evr.item()
            
            n_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰æŒ‡æ ‡
            pbar.set_postfix({
                'mse': f'{mse.item():.4f}',
                'rel_f': f'{rel_f.item():.4f}',
                'evr': f'{evr.item():.4f}'
            })
    
    pbar.close()
    return {
        "mse": total_mse / max(n_batches, 1),
        "rel_f": total_rel_f / max(n_batches, 1),
        "evr": total_evr / max(n_batches, 1),
    }


def save_history_json(history, path):
    """ä¿å­˜è®­ç»ƒå†å²ä¸ºJSON"""
    import json
    with open(path, 'w') as f:
        json.dump(history, f, indent=2)


def plot_curves(history, results_dir):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    import matplotlib.pyplot as plt
    
    epochs = [log['epoch'] for log in history]
    train_losses = [log['train_loss'] for log in history]
    val_mses = [log['val_mse'] for log in history]
    rel_f_scores = [log['val_rel_f'] for log in history]
    evr_scores = [log['val_evr'] for log in history]
    
    # æŸå¤±æ›²çº¿
    plt.figure(figsize=(15, 5))
    
    # ç¬¬ä¸€ä¸ªå­å›¾ï¼šè®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±åœ¨åŒä¸€å¼ å›¾ä¸Š
    plt.subplot(1, 3, 1)
    plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss', marker='o', markersize=4)
    plt.plot(epochs, val_mses, 'r-', linewidth=2, label='Val MSE', marker='s', markersize=4)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training & Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ç¬¬äºŒä¸ªå­å›¾ï¼šéªŒè¯MSEï¼ˆä¿æŒåŸæ ·ï¼‰
    plt.subplot(1, 3, 2)
    plt.plot(epochs, val_mses, 'r-', linewidth=2, label='Val MSE', marker='s', markersize=4)
    plt.xlabel('Epoch')
    plt.ylabel('MSE')
    plt.title('Validation MSE')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ç¬¬ä¸‰ä¸ªå­å›¾ï¼šRel_Få’ŒEVRï¼ˆä¿æŒåŸæ ·ï¼‰
    plt.subplot(1, 3, 3)
    plt.plot(epochs, rel_f_scores, 'g-', linewidth=2, label='Rel_F', marker='^', markersize=4)
    plt.plot(epochs, evr_scores, 'm-', linewidth=2, label='EVR', marker='d', markersize=4)
    plt.xlabel('Epoch')
    plt.ylabel('Score')
    plt.title('Rel_F and EVR')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'loss_curves.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    # å•ç‹¬çš„æ›²çº¿
    # è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±å¯¹æ¯”å›¾
    plt.figure(figsize=(12, 8))
    plt.plot(epochs, train_losses, 'b-', linewidth=3, label='Train Loss', marker='o', markersize=6)
    plt.plot(epochs, val_mses, 'r-', linewidth=3, label='Val MSE', marker='s', markersize=6)
    plt.xlabel('Epoch', fontsize=14)
    plt.ylabel('Loss', fontsize=14)
    plt.title('Training vs Validation Loss Comparison', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    
    # æ·»åŠ ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    train_final = train_losses[-1] if train_losses else 0
    val_final = val_mses[-1] if val_mses else 0
    plt.text(0.02, 0.98, f'Final Train Loss: {train_final:.6f}', 
             transform=plt.gca().transAxes, fontsize=10, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    plt.text(0.02, 0.92, f'Final Val MSE: {val_final:.6f}', 
             transform=plt.gca().transAxes, fontsize=10, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'train_val_loss_comparison.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    # Rel_Fæ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, rel_f_scores, 'g-', linewidth=2, label='Rel_F', marker='^', markersize=4)
    plt.xlabel('Epoch')
    plt.ylabel('Rel_F Score')
    plt.title('Relative F-norm Score')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(results_dir, 'relF_curve.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    # EVRæ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, evr_scores, 'm-', linewidth=2, label='EVR', marker='d', markersize=4)
    plt.xlabel('Epoch')
    plt.ylabel('EVR Score')
    plt.title('Eigenvalue Ratio Score')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(results_dir, 'evr_curve.png'), dpi=150, bbox_inches='tight')
    plt.close()


def save_error_heatmap(error_matrix, path):
    """ä¿å­˜è¯¯å·®çƒ­åŠ›å›¾"""
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(10, 8))
    plt.imshow(error_matrix.cpu().numpy(), cmap='Reds', aspect='auto')
    plt.colorbar(label='Error Magnitude')
    plt.title('Reconstruction Error Heatmap')
    plt.xlabel('Feature Dimension')
    plt.ylabel('Token Position')
    plt.savefig(path, dpi=150, bbox_inches='tight')
    plt.close()


def save_B_row_norm_hist(B_matrix, path):
    """ä¿å­˜BçŸ©é˜µè¡ŒèŒƒæ•°åˆ†å¸ƒç›´æ–¹å›¾"""
    import matplotlib.pyplot as plt
    
    B_norms = torch.norm(B_matrix.squeeze(0), dim=1).cpu().numpy()
    
    plt.figure(figsize=(8, 6))
    plt.hist(B_norms, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    plt.xlabel('Row Norm')
    plt.ylabel('Frequency')
    plt.title('B Matrix Row Norm Distribution')
    plt.grid(True, alpha=0.3)
    plt.savefig(path, dpi=150, bbox_inches='tight')
    plt.close()


def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='Train low-rank decomposition model with real QwenVL data')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for training')
    parser.add_argument('--epochs', type=int, default=20, help='Number of training epochs')
    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--rank', type=int, default=128, help='Rank for low-rank decomposition')
    parser.add_argument('--lambda_B', type=float, default=1e-4, help='Weight for B regularization')
    parser.add_argument('--lambda_ortho', type=float, default=1e-3, help='Weight for orthogonality loss')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight decay')
    parser.add_argument('--results_dir', type=str, default='results', help='Directory to save results')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--patience', type=int, default=10, help='Early stopping patience (epochs)')
    
    args = parser.parse_args()
    
    set_seed(args.seed)
    # å¼ºåˆ¶ä½¿ç”¨GPUè®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸš€ Using GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸš€ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        device = torch.device("cpu")
        print("âš ï¸  No GPU available, using CPU")

    # è¶…å‚æ•°
    T = 256  # 256 tokens for 16x16 spatial layout
    D = 2048  # 2048 hidden dimensions
    r = args.rank
    batch_size = args.batch_size
    num_epochs = args.epochs
    learning_rate = args.lr
    weight_decay = args.weight_decay
    lambda_B = args.lambda_B
    lambda_ortho = args.lambda_ortho
    results_dir = args.results_dir
    os.makedirs(results_dir, exist_ok=True)

    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"Configuration:")
    print(f"  Image size: 448x448 (16x16 patches)")
    print(f"  Token count: {T}")
    print(f"  Feature dimension: {D}")
    print(f"  Rank: {r}")
    print(f"  Batch size: {batch_size}")
    print(f"  Learning rate: {learning_rate}")
    print(f"  Device: {device}")

    # åŠ è½½çœŸå®æ•°æ®é›†ï¼ˆä½¿ç”¨çœŸå®çš„QwenVL3Bæ¨¡å‹ï¼‰
    print("\nLoading textVQA dataset with real QwenVL3B model...")
    try:
        dataset = RealQwenVLDataset(
            dataset_dir="/data2/dzr/textVQA_groundingtask_bbox",
            image_size=448,
            use_cache=True,
            cache_dir="./feature_cache_real"
        )
        print(f"âœ… Dataset loaded successfully with {len(dataset)} samples")
        
        # æ£€æŸ¥ç‰¹å¾çŸ©é˜µç»´åº¦
        print(f"ğŸ” Dataset features shape: {dataset.features.shape}")
        print(f"ğŸ” Dataset features dtype: {dataset.features.dtype}")
        print(f"ğŸ” Dataset features device: {dataset.features.device}")
        
        # éªŒè¯ç‰¹å¾çŸ©é˜µç»´åº¦
        expected_shape = (len(dataset), 256, 2048)
        if dataset.features.shape != expected_shape:
            print(f"âš ï¸  WARNING: Unexpected feature shape!")
            print(f"   Expected: {expected_shape}")
            print(f"   Actual: {dataset.features.shape}")
            raise ValueError(f"Feature matrix shape mismatch: {dataset.features.shape} != {expected_shape}")
        else:
            print(f"âœ… Feature matrix shape validation passed!")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (80%, 10%, 10% åˆ’åˆ†)
        train_loader, val_loader, test_loader = create_data_loaders_real(
            dataset, 
            batch_size=batch_size,
            train_ratio=0.8,
            val_ratio=0.1,
            test_ratio=0.1,
            num_workers=4
        )
        print(f"âœ… Data loaders created successfully")
        
        # æµ‹è¯•ç¬¬ä¸€ä¸ªbatch
        print("\nğŸ” Testing first batch from train loader...")
        for batch in train_loader:
            X = batch[0]
            print(f"ğŸ” First batch X shape: {X.shape}")
            print(f"ğŸ” First batch X dtype: {X.dtype}")
            print(f"ğŸ” First batch X device: {X.device}")
            break
        
    except Exception as e:
        print(f"âŒ Failed to load dataset: {e}")
        print("Falling back to dummy dataset...")
        
        # Fallback: ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        from train import generate_dummy_dataset
        dataset = generate_dummy_dataset(2000, T, D)
        val_size = int(len(dataset) * 0.1)
        test_size = int(len(dataset) * 0.1)
        train_size = len(dataset) - val_size - test_size
        
        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size, test_size]
        )
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
        
        print(f"âœ… Dummy dataset created with {len(dataset)} samples")

    # åˆ›å»ºæ¨¡å‹
    model = LowRankProjSymmetric(input_dim=D, num_tokens=T, rank=r, normalize=False)
    model.to(device)
    print(f"âœ… Model created and moved to {device}")

    # åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

    best_val = math.inf
    best_path = os.path.join(results_dir, "best_model.pt")
    history = []
    
    # æ—©åœæœºåˆ¶
    patience_counter = 0
    best_epoch = 0

    print(f"\nStart training with early stopping (patience: {args.patience})...")
    for epoch in range(1, num_epochs + 1):
        t0 = time.time()
        train_stats = train_one_epoch(model, train_loader, optimizer, device, lambda_B, lambda_ortho)
        val_stats = evaluate(model, val_loader, device)
        scheduler.step(val_stats["mse"])
        epoch_time = time.time() - t0

        log = {
            "epoch": epoch,
            "train_loss": train_stats["loss"],
            "train_mse": train_stats["mse"],
            "val_mse": val_stats["mse"],
            "val_rel_f": val_stats["rel_f"],
            "val_evr": val_stats["evr"],
            "lr": optimizer.param_groups[0]["lr"],
            "time": epoch_time,
        }
        history.append(log)
        print(f"Epoch {epoch:03d} | train_loss={log['train_loss']:.6f} train_mse={log['train_mse']:.6f} "
              f"val_mse={log['val_mse']:.6f} rel_f={log['val_rel_f']:.6f} evr={log['val_evr']:.6f} "
              f"lr={log['lr']:.2e} time={epoch_time:.1f}s")

        # checkpoint
        if val_stats["mse"] < best_val:
            best_val = val_stats["mse"]
            best_epoch = epoch
            patience_counter = 0  # é‡ç½®è€å¿ƒè®¡æ•°å™¨
            # ä¿å­˜å¸¦æœ‰epochå·çš„æƒé‡æ–‡ä»¶
            epoch_best_path = os.path.join(results_dir, f"best_model_epoch_{epoch:03d}.pt")
            torch.save({
                "model_state": model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
                "config": {
                    "T": T, "D": D, "rank": r,
                    "lambda_B": lambda_B, "lambda_ortho": lambda_ortho,
                    "image_size": 448, "patch_size": 16,
                    "dataset": "textVQA_groundingtask_bbox",
                    "data_split": "80-10-10",
                    "feature_extractor": "QwenVL3B_real",
                },
                "history": history,
                "epoch": epoch,
                "best_val_mse": best_val,
            }, epoch_best_path)
            # åŒæ­¥æ›´æ–°ä¸€ä¸ªå›ºå®šåç§°çš„è½¯é“¾æ¥/å‰¯æœ¬
            try:
                # ä¼˜å…ˆå°è¯•ç¬¦å·é“¾æ¥æ›´æ–°
                if os.path.islink(best_path) or os.path.exists(best_path):
                    os.remove(best_path)
                os.symlink(os.path.basename(epoch_best_path), best_path)
            except Exception:
                # è‹¥ç¬¦å·é“¾æ¥ä¸å¯ç”¨ï¼Œå¤åˆ¶ä¸€ä»½
                shutil.copyfile(epoch_best_path, best_path)
            print(f"âœ… New best checkpoint saved at epoch {epoch}: {os.path.basename(epoch_best_path)} (Val MSE: {best_val:.6f})")
        else:
            patience_counter += 1
            print(f"âš ï¸  No improvement for {patience_counter} epochs (best: {best_val:.6f} at epoch {best_epoch})")
            
        # æ—©åœæ£€æŸ¥
        if patience_counter >= args.patience:
            print(f"ğŸ›‘ Early stopping triggered after {patience_counter} epochs without improvement")
            print(f"Best validation MSE: {best_val:.6f} at epoch {best_epoch}")
            break

    # éªŒè¯é›†ä¸ŠæŠ½æ ·SVDä¸‹ç•Œè¯„ä¼°
    if len(val_loader.dataset) > 0:
        print("\nEvaluating SVD lower bound...")
        sample_loader = DataLoader(val_loader.dataset, batch_size=1, shuffle=True)
        default_samples = min(10, len(val_loader.dataset))
        svd_samples = int(os.environ.get("SVD_SAMPLES", default_samples))
        errs = []
        count = 0
        with torch.no_grad():
            for (X,) in sample_loader:
                X = layernorm_along_feature(X)  # ä¸æ¨¡å‹ä¸€è‡´çš„é¢„å¤„ç†
                e = svd_truncated_error(X.squeeze(0), rank=r)
                errs.append(e)
                count += 1
                if count >= svd_samples:
                    break
        if errs:
            avg_svd_err = sum(errs) / len(errs)
            print(f"SVD lower-bound relative error (avg over {len(errs)} samples): {avg_svd_err:.6f}")

    # æµ‹è¯•é›†è¯„ä¼°
    if test_loader is not None:
        print("\nEvaluating on test set...")
        test_stats = evaluate(model, test_loader, device)
        print(f"Test set - MSE: {test_stats['mse']:.6f}, Rel_F: {test_stats['rel_f']:.6f}, EVR: {test_stats['evr']:.6f}")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_results = {
            "test_mse": test_stats["mse"],
            "test_rel_f": test_stats["rel_f"],
            "test_evr": test_stats["evr"],
        }
        import json
        with open(os.path.join(results_dir, 'test_results.json'), 'w') as f:
            json.dump(test_results, f, indent=2)

    # ä¿å­˜æ›²çº¿ä¸history
    save_history_json(history, os.path.join(results_dir, 'history.json'))
    if history:
        plot_curves(history, results_dir)

    # è¯¯å·®çƒ­åŠ›å›¾ä¸Bè¡ŒèŒƒæ•°åˆ†å¸ƒï¼ˆå–ä¸€å°æ‰¹æ¬¡æ ·æœ¬ï¼‰
    print("\nGenerating visualizations...")
    with torch.no_grad():
        sample_loader = DataLoader(train_loader.dataset, batch_size=1, shuffle=True)
        for (X,) in sample_loader:
            X = layernorm_along_feature(X.to(device))
            X_hat, A, B = model(X)
            err_mat = (X - X_hat).squeeze(0)
            save_error_heatmap(err_mat, os.path.join(results_dir, 'error_heatmap.png'))
            # B: (1, r, D) -> å–ç¬¬ä¸€ä¸ªæ ·æœ¬
            save_B_row_norm_hist(B, os.path.join(results_dir, 'B_row_norm_hist.png'))
            break

    print(f"\nâœ… Training completed!")
    print(f"Best validation MSE: {best_val:.6f}")
    print(f"Results saved to: {results_dir}")


if __name__ == "__main__":
    main()
