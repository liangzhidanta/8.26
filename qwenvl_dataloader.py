#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨çœŸå®QwenVL3Bæ¨¡å‹çš„æ•°æ®é›†åŠ è½½å™¨
æŒ‰ç…§test.ipynbä¸­çš„æ–¹å¼è·å–å›¾åƒç‰¹å¾
ä¸“é—¨ä½¿ç”¨Qwen2.5-VL-3Bæ¨¡å‹
"""

import os
import torch
import pandas as pd
import numpy as np
from PIL import Image
import io
from torch.utils.data import Dataset, DataLoader, random_split
from typing import List, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["MODELSCOPE_CACHE"] = "/data2/dzr/.cache" 
os.environ["TOKENS_PARALLELISM"] = "false"

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from modelscope import AutoProcessor, AutoModel
    from qwen_vl_utils import process_vision_info
    print("Using ModelScope for model loading")
    MODEL_LOADER = "modelscope"
except ImportError as e:
    print(f"Warning: {e}")
    print("Trying alternative imports...")
    try:
        from transformers import AutoProcessor, AutoModel
        # ç®€å•çš„fallbackå®ç°
        def process_vision_info(messages):
            image_inputs = []
            for msg in messages:
                for content in msg["content"]:
                    if content["type"] == "image":
                        image_inputs.append(content["image"])
            return image_inputs, []
        print("Using Transformers as fallback")
        MODEL_LOADER = "transformers"
    except ImportError as e2:
        print(f"Error: {e2}")
        raise


class RealQwenVLDataset(Dataset):
    """ä½¿ç”¨çœŸå®QwenVL3Bæ¨¡å‹çš„textVQAæ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, 
                 dataset_dir: str = "/data2/dzr/textVQA_groundingtask_bbox",
                 image_size: int = 448,
                 use_cache: bool = True,
                 cache_dir: str = "./feature_cache_real"):
        """
        Args:
            dataset_dir: æ•°æ®é›†ç›®å½•
            image_size: å›¾åƒå°ºå¯¸
            use_cache: æ˜¯å¦ä½¿ç”¨ç‰¹å¾ç¼“å­˜
            cache_dir: ç‰¹å¾ç¼“å­˜ç›®å½•
        """
        self.dataset_dir = dataset_dir
        self.image_size = image_size
        self.use_cache = use_cache
        self.cache_dir = cache_dir
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if use_cache:
            os.makedirs(cache_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®é›†
        self._load_dataset()
        
        # åˆå§‹åŒ–QwenVLæ¨¡å‹å’Œå¤„ç†å™¨
        self._init_model()
        
        # é¢„å¤„ç†æ•°æ®
        self._preprocess_data()
    
    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†æ–‡ä»¶"""
        print("Loading textVQA dataset...")
        
        # è®­ç»ƒæ•°æ®æ–‡ä»¶
        train_files = [
            "data/train-00000-of-00003-d0560b4d0b4feac4.parquet",
            "data/train-00001-of-00003-0085a9b480196c92.parquet", 
            "data/train-00002-of-00003-ebe81cbdb50b653b.parquet"
        ]
        
        # åŠ è½½æ‰€æœ‰è®­ç»ƒæ•°æ®
        train_data = []
        for file_path in train_files:
            full_path = os.path.join(self.dataset_dir, file_path)
            if os.path.exists(full_path):
                print(f"Loading file: {file_path}")
                df = pd.read_parquet(full_path)
                train_data.append(df)
                print(f"  - Samples: {len(df)}")
        
        # åˆå¹¶æ•°æ®
        if train_data:
            self.df = pd.concat(train_data, ignore_index=True)
            print(f"âœ… Dataset loaded successfully! Total samples: {len(self.df)}")
        else:
            raise FileNotFoundError("No training data files found")
    
    def _init_model(self):
        """åˆå§‹åŒ–QwenVLæ¨¡å‹å’Œå¤„ç†å™¨"""
        print("Initializing QwenVL model...")
        
        try:
            # ä¸“é—¨ä½¿ç”¨Qwen2.5-VL-3B-Instructæ¨¡å‹
            model_name = "Qwen/Qwen2.5-VL-3B-Instruct"
            print(f"Loading model: {model_name}")
            
            self.processor = AutoProcessor.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(
                model_name, 
                torch_dtype=torch.bfloat16, 
                device_map="auto"
            )
            print("âœ… Qwen2.5-VL-3B-Instruct model loaded successfully")
            
        except Exception as e:
            print(f"Error: Failed to load Qwen2.5-VL-3B-Instruct: {e}")
            print("This model is required for the project. Please check:")
            print("1. Internet connection")
            print("2. Model availability")
            print("3. Transformers version compatibility")
            raise RuntimeError(f"Failed to load required model: {model_name}")
        
        # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        # è·å–è®¾å¤‡ä¿¡æ¯
        self.device = next(self.model.parameters()).device
        print(f"Model device: {self.device}")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰get_image_featuresæ–¹æ³•
        if not hasattr(self.model, 'get_image_features'):
            print("Warning: Model doesn't have get_image_features method")
            print("Available methods:", [method for method in dir(self.model) if not method.startswith('_')])
            print("This may cause issues with feature extraction.")
    
    def _bytes_to_image(self, image_bytes) -> Optional[Image.Image]:
        """å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºPILå›¾åƒ"""
        try:
            if isinstance(image_bytes, dict) and 'bytes' in image_bytes:
                img_data = image_bytes['bytes']
            else:
                img_data = image_bytes
            
            image = Image.open(io.BytesIO(img_data)).convert('RGB')
            return image
        except Exception as e:
            print(f"Image conversion failed: {e}")
            return None
    
    def _resize_image(self, image: Image.Image) -> Image.Image:
        """å°†å›¾åƒresizeåˆ°æŒ‡å®šå°ºå¯¸"""
        return image.resize((self.image_size, self.image_size), Image.Resampling.LANCZOS)
    
    def _get_image_features(self, image: Image.Image) -> torch.Tensor:
        """ä½¿ç”¨QwenVLè·å–å›¾åƒç‰¹å¾ï¼ŒæŒ‰ç…§test.ipynbçš„æ–¹å¼"""
        try:
            # æŒ‰ç…§test.ipynbçš„æ–¹å¼å‡†å¤‡æ¶ˆæ¯å’Œè¾“å…¥
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": image,
                        },
                        {"type": "text", "text": "Describe this image."},
                    ],
                }
            ]
            
            # å‡†å¤‡æ¨ç†è¾“å…¥
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            try:
                # ä½¿ç”¨process_vision_info
                image_inputs, video_inputs = process_vision_info(messages)
            except Exception as e:
                print(f"process_vision_info failed: {e}, using fallback")
                # fallbackå¤„ç†
                image_inputs = [image]
                video_inputs = []
            
            # ä½¿ç”¨å¤„ç†å™¨å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            
            # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
            inputs = {k: v.to(self.device) if hasattr(v, 'to') else v for k, v in inputs.items()}
            
            # ä»å¤„ç†åçš„è¾“å…¥ä¸­è·å– pixel_values å’Œ image_grid_thw
            pixel_values = inputs['pixel_values'].to(self.model.dtype)
            image_grid_thw = inputs.get('image_grid_thw', torch.tensor([[1, 1, 1]]).to(self.device))
            if not isinstance(image_grid_thw, torch.Tensor):
                image_grid_thw = torch.tensor(image_grid_thw).to(self.device)
            image_grid_thw = image_grid_thw.to(torch.long)
            
            # é€šè¿‡æ¨¡å‹çš„ get_image_features æ–¹æ³•ç¼–ç å›¾åƒ
            with torch.no_grad():
                try:
                    image_features_list = self.model.get_image_features(pixel_values, image_grid_thw=image_grid_thw)
                    image_features = torch.cat(image_features_list, dim=0)
                except AttributeError:
                    print("Model doesn't have get_image_features method, trying alternative...")
                    # å¦‚æœæ²¡æœ‰get_image_featuresæ–¹æ³•ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                    # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–ç‰¹å¾æå–æ–¹æ³•
                    raise RuntimeError("Model doesn't support get_image_features")
                
                except Exception as e:
                    print(f"Feature extraction failed: {e}")
                    raise
            
            # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡® [batch_size, 256, 2048]
            if len(image_features.shape) == 2:
                # å¦‚æœæ˜¯ [256, 2048]ï¼Œæ·»åŠ batchç»´åº¦
                image_features = image_features.unsqueeze(0)
            
            return image_features
            
        except Exception as e:
            print(f"Feature extraction failed: {e}")
            raise
    
    def _preprocess_data(self):
        """é¢„å¤„ç†æ•°æ®ï¼Œæå–å›¾åƒç‰¹å¾"""
        print("Preprocessing data and extracting features...")
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„ç‰¹å¾
        if self.use_cache and os.path.exists(self.cache_dir):
            # æ£€æŸ¥ç¼“å­˜ç›®å½•ä¸­çš„ç‰¹å¾æ–‡ä»¶æ•°é‡
            cache_files = [f for f in os.listdir(self.cache_dir) if f.startswith('features_') and f.endswith('.pt')]
            if len(cache_files) == len(self.df):
                # æ£€æŸ¥ç¬¬ä¸€ä¸ªç‰¹å¾æ–‡ä»¶çš„å½¢çŠ¶
                try:
                    first_cache_path = os.path.join(self.cache_dir, 'features_0.pt')
                    if os.path.exists(first_cache_path):
                        first_features = torch.load(first_cache_path, map_location='cpu')
                        if first_features.shape == (1, 256, 2048):
                            print(f"âœ… Found complete feature cache with {len(cache_files)} samples")
                            print(f"âœ… First feature shape: {first_features.shape}, dtype: {first_features.dtype}")
                            print("ğŸš€ Skipping image processing, loading all cached features...")
                            
                            # ç›´æ¥åŠ è½½æ‰€æœ‰ç¼“å­˜çš„ç‰¹å¾
                            self.features = []
                            self.valid_indices = []
                            
                            for idx in range(len(self.df)):
                                cache_path = os.path.join(self.cache_dir, f"features_{idx}.pt")
                                try:
                                    features = torch.load(cache_path, map_location='cpu')
                                    self.features.append(features)
                                    self.valid_indices.append(idx)
                                except Exception as e:
                                    print(f"Failed to load cached features for sample {idx}: {e}")
                                    continue
                            
                            print(f"âœ… Loaded {len(self.features)} cached features")
                            
                            # è½¬æ¢ä¸ºtensor
                            if self.features:
                                self.features = torch.cat(self.features, dim=0)  # [N, 256, 2048]
                                print(f"âœ… Final features tensor shape: {self.features.shape}")
                                print(f"âœ… Final features tensor dtype: {self.features.dtype}")
                                print(f"âœ… Final features tensor device: {self.features.device}")
                            else:
                                raise RuntimeError("No valid features loaded from cache")
                            
                            return  # ç›´æ¥è¿”å›ï¼Œè·³è¿‡åç»­çš„å›¾åƒå¤„ç†é€»è¾‘
                        else:
                            print(f"âš ï¸  First cached feature has wrong shape: {first_features.shape}, expected (1, 256, 2048)")
                    else:
                        print("âš ï¸  First cache file not found")
                except Exception as e:
                    print(f"âš ï¸  Error checking first cache file: {e}")
                print("ğŸ”„ Proceeding with normal image processing...")
        
        self.features = []
        self.valid_indices = []
        
        for idx, row in self.df.iterrows():
            if idx % 100 == 0:
                print(f"Processing sample {idx}/{len(self.df)}")
            
            # è·å–å›¾åƒ
            image = self._bytes_to_image(row['image'])
            if image is None:
                continue
            
            # Resizeå›¾åƒ
            image = self._resize_image(image)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_path = None
            if self.use_cache:
                cache_path = os.path.join(self.cache_dir, f"features_{idx}.pt")
                if os.path.exists(cache_path):
                    try:
                        features = torch.load(cache_path, map_location='cpu')  # åŠ è½½åˆ°CPU
                        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ç¼“å­˜çš„ç‰¹å¾ç»´åº¦
                        if idx < 5:  # åªæ‰“å°å‰5ä¸ªæ ·æœ¬çš„è°ƒè¯•ä¿¡æ¯
                            print(f"ğŸ” DEBUG: Loaded cached features {idx}: shape={features.shape}, dtype={features.dtype}")
                        self.features.append(features)
                        self.valid_indices.append(idx)
                        continue
                    except Exception as e:
                        print(f"Failed to load cached features for sample {idx}: {e}")
                        pass
            
            # æå–ç‰¹å¾
            try:
                features = self._get_image_features(image)
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥åŸå§‹ç‰¹å¾ç»´åº¦
                if idx < 5:
                    print(f"ğŸ” DEBUG: Raw features {idx}: shape={features.shape}, dtype={features.dtype}")
                
                # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡® [1, 256, 2048]
                if len(features.shape) == 2:
                    features = features.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                elif len(features.shape) == 3 and features.shape[0] != 1:
                    # å¦‚æœbatchç»´åº¦ä¸æ˜¯1ï¼Œå–ç¬¬ä¸€ä¸ª
                    features = features[:1]
                
                # å°†ç‰¹å¾ç§»åˆ°CPUå¹¶è½¬æ¢ä¸ºfloat32ä»¥é¿å…pin_memoryé—®é¢˜
                features = features.cpu().to(torch.float32)
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å¤„ç†åçš„ç‰¹å¾ç»´åº¦
                if idx < 5:
                    print(f"ğŸ” DEBUG: Processed features {idx}: shape={features.shape}, dtype={features.dtype}")
                
                # ä¿å­˜åˆ°ç¼“å­˜
                if self.use_cache and cache_path:
                    torch.save(features, cache_path)
                
                self.features.append(features)
                self.valid_indices.append(idx)
                
            except Exception as e:
                print(f"Failed to extract features for sample {idx}: {e}")
                continue
        
        print(f"âœ… Feature extraction completed! Valid samples: {len(self.features)}")
        
        # è½¬æ¢ä¸ºtensor
        if self.features:
            # æ£€æŸ¥æ‰€æœ‰ç‰¹å¾çš„ç»´åº¦ä¸€è‡´æ€§
            feature_shapes = [f.shape for f in self.features]
            unique_shapes = set(feature_shapes)
            print(f"ğŸ” DEBUG: Feature shapes found: {unique_shapes}")
            
            if len(unique_shapes) > 1:
                print(f"âš ï¸  WARNING: Inconsistent feature shapes detected!")
                print(f"   Expected: torch.Size([1, 256, 2048])")
                print(f"   Found: {unique_shapes}")
                
                # ç»Ÿä¸€æ‰€æœ‰ç‰¹å¾åˆ°æ ‡å‡†ç»´åº¦
                print("ğŸ”§ Fixing feature dimensions...")
                standardized_features = []
                for i, f in enumerate(self.features):
                    if f.shape != (1, 256, 2048):
                        if len(f.shape) == 2:
                            f = f.unsqueeze(0)
                        elif f.shape[0] > 1:
                            f = f[:1]
                        if f.shape[1] != 256 or f.shape[2] != 2048:
                            print(f"âš ï¸  Sample {i} has wrong dimensions: {f.shape}, skipping...")
                            continue
                        f = f.to(torch.float32)
                    standardized_features.append(f)
                
                self.features = standardized_features
                print(f"âœ… Standardized features: {len(self.features)} samples")
            
            self.features = torch.cat(self.features, dim=0)  # [N, 256, 2048]
            print(f"âœ… Final features tensor shape: {self.features.shape}")
            print(f"âœ… Final features tensor dtype: {self.features.dtype}")
            print(f"âœ… Final features tensor device: {self.features.device}")
        else:
            raise RuntimeError("No valid features extracted")
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        """è¿”å›ç‰¹å¾çŸ©é˜µ"""
        return (self.features[idx],)  # è¿”å›tupleä»¥å…¼å®¹ç°æœ‰ä»£ç 
    
    def get_split_datasets(self, train_ratio: float = 0.8, val_ratio: float = 0.1, test_ratio: float = 0.1):
        """åˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†"""
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "Ratios must sum to 1.0"
        
        total_size = len(self)
        train_size = int(total_size * train_ratio)
        val_size = int(total_size * val_ratio)
        test_size = total_size - train_size - val_size
        
        print(f"Dataset split: Train={train_size}, Val={val_size}, Test={test_size}")
        
        train_set, val_set, test_set = random_split(
            self, [train_size, val_size, test_size]
        )
        
        return train_set, val_set, test_set


def create_data_loaders_real(dataset: RealQwenVLDataset, 
                            batch_size: int = 8,
                            train_ratio: float = 0.8,
                            val_ratio: float = 0.1,
                            test_ratio: float = 0.1,
                            num_workers: int = 4) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    
    # åˆ’åˆ†æ•°æ®é›†
    train_set, val_set, test_set = dataset.get_split_datasets(train_ratio, val_ratio, test_ratio)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_set, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=num_workers,
        pin_memory=True  # å¯ç”¨pin_memoryä»¥åŠ é€ŸGPUè®­ç»ƒ
    )
    
    val_loader = DataLoader(
        val_set, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True  # å¯ç”¨pin_memoryä»¥åŠ é€ŸGPUè®­ç»ƒ
    )
    
    test_loader = DataLoader(
        test_set, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True  # å¯ç”¨pin_memoryä»¥åŠ é€ŸGPUè®­ç»ƒ
    )
    
    return train_loader, val_loader, test_loader


if __name__ == "__main__":
    # æµ‹è¯•çœŸå®QwenVLæ•°æ®é›†åŠ è½½å™¨
    print("Testing real QwenVL dataset loader...")
    
    try:
        dataset = RealQwenVLDataset(use_cache=True)
        print(f"Dataset loaded with {len(dataset)} samples")
        
        # æµ‹è¯•ç‰¹å¾æå–
        sample_features = dataset[0]
        print(f"Sample features shape: {sample_features[0].shape}")
        
        # æµ‹è¯•æ•°æ®åˆ’åˆ†
        train_set, val_set, test_set = dataset.get_split_datasets()
        print(f"Split sizes: Train={len(train_set)}, Val={len(val_set)}, Test={len(test_set)}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = create_data_loaders_real(dataset, batch_size=4)
        print(f"Data loaders created successfully")
        
        # æµ‹è¯•ä¸€ä¸ªbatch
        for batch in train_loader:
            X = batch[0]
            print(f"Batch shape: {X.shape}")
            break
            
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()

